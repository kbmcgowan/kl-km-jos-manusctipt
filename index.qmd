---
title: "Removing the disguise: the matched guise technique, incongruity, and listener awareness"
author:
  - name: "Kyler Laycock"
    orcid: 0000-0002-3731-0841
    corresponding: true
    email: laycock.21@buckeyemail.osu.edu
    affiliations: "The Ohio State University"
  - name: "&nbsp;Kevin B. McGowan"
    orcid: 0000-0002-8214-1901
    corresponding: false
    email: kbmcgowan@uky.edu
    affiliations: "University of Kentucky"
keywords:
  - awareness
  - control
  - gender
  - inverted matched guise
  - sociophonetic perception
abstract: |
  Sociophonetic perception is often studied using versions of the matched guise technique. Linguists using this technique appear united in the methodological assumptions that participants believe the manipulation and that this belief influences perception below the level of introspective awareness. We report an audiovisual matched guise experiment with a novel 'unhidden' instruction condition. The basic task is a replication of the Strand effect [@strandJohnson1996; @strand1999]. Participants in the 'unhidden' condition were instructed that the man or woman in the photo did not represent the voice they were listening to. Participants in both guises exhibited the Strand effect to nearly numerically identical extents. This result suggests that participants need not believe a link exists between a voice and a purported social category for visually-cued social information to influence segmental perception. We explore the implications of this result for the MGT and for theories of social awareness and speech perception more broadly.

date: last-modified
bibliography: jos-laycock-mcgowan.bib
citation:
  container-title: Awareness and Control of Sociolinguistic Variation
number-sections: false
suppress-title-page: true
pdf-engine: xelatex
pdf-engine-opts:
  - '--no-shell-escape'
  - '--halt-on-error'
format:
  html:
    theme: simplex
    fontsize: 1.2em
    linestretch: 1.7
    mainfont: Doulos SIL, Noto Serif, Helvetica Neue, Helvetica, Arial, sans
    monofont: Noto Sans Mono, Cascadia Mono, Menlo, Consolas, Courier New, Courier
    backgroundcolor: "white"
    fontcolor: "black"
  docx:
    toc: false
    number-sections: false
    highlight-style: github
    reference-doc: custom-reference-doc.docx
  pdf:
    toc: false
    number-sections: false
    colorlinks: true
    mainfont: "Doulos SIL"
editor: 
  markdown:
    wrap: 72
---

# Introduction {#sec-intro}

There is abundant, converging evidence from experimental, ethnographic, and sociocultural approaches to the study of language that gender is performed by speakers and perceived by interlocutors through a stylistic bricolage [@zimman2017] comprising both non-linguistic and linguistic resources [@barrett2014; @bucholtz2002]. Gender is a culturally-situated practice, and, crucially, social meaning is performed by embodied voices that simultaneously produce the distinctions necessary for both social and linguistic meaning [@hall2021language; @podesvaKajino2014; @bucholtzHall2016; @sumner2014]. This intersection of the construction of social and linguistic meaning via precise, dynamic speech articulation is perhaps nowhere more evident than in the palato-alveolar and alveolar fricative categories, [ʃ] and [s]; the first segments in words like *ship* and *sip* [@calder2018; @mackMunson2012b; @pharao2014; @strand1999].

There is little consensus, however, around the extent to which language users are aware of, and can control, these fine gradations of social meaning in production and perception. In the context of this chapter we are using 'awareness' to refer to explicit, conscious awareness of the tripartite relationship between a social label, its phonetic reflexes, and the connections between these [@BabelIssue; @donofrio2021; @bakhtin1981]. The cognitive reality of this tripartite relationship between the concepts of gender identities and instances of fine phonetic detail is essential for the performance of those identities. This observation holds regardless of speaker and listener awareness. It even holds if what the listener believes about the speaker is false; a monolingual American listener might expect a Beijing voice to be non-rhotic [@mcgowan2016], Japanese women to use final particles [@inoue2003], or a gay male voice to have a lisp [@mackmunson2012]. Expectations need not be accurate to shape perception [@preston1996].

Relatedly, one can *control*, in production, the phonetics of one's gender without explicit acknowledgement or introspective awareness that one is doing so or what those details might be [@laver1968]. Indeed, children as young as 4, well before any effects of puberty might have arrived, can do precisely this [@perryOhdeAshmead2001] and many of our own college students, when first confronted with the idea that they participate in the social construction of gender through the fine phonetic details of their speech will respond with real, sometimes agitated, disbelief. Even trained, experienced sociolinguists and phoneticians tend to conceive of the fundamental frequency of voiced speech sounds as the primary, biological phonetic detail associated with gender performance [@foulkesDocherty2006,  p. 411]; but this cue is neither necessary nor sufficient for the production and perception of gender identity [@zimman2017; @johnson2005].

In perception the concept of control is less intuitive. Here we owe much of our general conceptualization of 'control' to [@BabelIssue]'s application of the semiotic role of the interpretant [@peirce1955] in perception and Preston's [-@preston1996; -@preston2016] four modes of awareness. Critical to our understanding of this phenomenon is the stipulation that the ability to 'perform' or to 'employ' the linking relationship between a social label and its phonetic reflexes is just as clearly a task for the listening subject as it is for the speaker. Social meaning making occurs in interaction [@SharmaIssue]; a listener must be able to control, that is to link, the auditory cues of a performed gender identity to the cognitive representation of that identity just as much as a speaker's vocal tract must be capable of the gestural control required to implement the phonetics of that identity if the tripartite, dialogic construction of identity in discourse is to occur. Again, none of this *requires* introspective awareness as perception and even attention are possible without awareness on the part of the perceiver [@craik_recognition_2015; @prinz_unconscious_2015; @graziano_attention_2015; @dehaene_towards_2001].

Clarifying these definitions and exploring their implications for the sociophonetic perception of gender is important because gender perception is a phenomenon that crosses disciplinary and subdisciplinary boundaries and approaches to language and social meaning. These varying disciplinary and subdisciplinary contexts employ quite different, sometimes contradictory, assumptions and theoretical commitments about the extent to which language users can bring aspects of perception into introspective awareness and control (conscious or otherwise). Even more than this, there are at least two, quite distinct, meanings in regular use for the word 'perception' [@drager2016a; @mcgowanBabel2020].

Perception is often construed as the processing of sensory input [cf. @evans2008, 'type 1' processing] into linguistic units like segments [@lisker1986; @pierrehumbert2003phonetic], speech gestures [@Fowler1986], and words [@gaskell2002representation; @Goldinger1998; @Johnson2006]. Perception, thus construed, is typically assumed to be automatic and to occur below the level of conscious awareness [@Joos1948, p. 63] and inaccessible to introspection even, in the case of subcategorical phonetic differences, by researchers themselves [@whalen1984]. Indeed, lack of awareness is taken as evidence of a "true perceptual phenomenon" for the McGurk effect, [@repp1982, p. 40], perceptual weighting of acoustic cues (p. 174), and phoneme restoration [@Ganong1980, p. 1].

The other meaning of perception in common use describes a higher-level, sometimes implicit, sometimes explicit, evaluative judgment of talkers and voices [cf. @evans2008, 'type 2' processing]. This is the meaning of perception employed in folk linguistics [@niedzielskiPreston2000] and perceptual dialectology [@cramer2021]. This is also the level of perception, for example, at which the sociolinguistic monitor is proposed by variationist sociolinguists to operate[^1] [@labovEtAl2011]. Importantly for the present study, this higher, evaluative level of perception is also the level for which the Matched Guise Technique (MGT) was originally developed.

[^1]: Although, in their response to Labov et al. 2011, [@levonFox2014] are careful to refer exclusively to *evaluation* rather than perception.

### Matched Guise: Perception, Evaluation, and Awareness {#sec-mgt}

In their foundational use of the MGT, @lambertEtAl1960 found that four bilingual Montrealers' voices evoked quite different social evaluations in their French vs. their English guises. Using the same talkers in both guises allowed researchers to control for "idiosyncratic settings of the voice" that might distract judges from the focus of the experiment [@laver1968]. Lambert et al. were clearly concerned that the evaluative judgments they sought were subject to listeners' subjective awareness; taking pains to deceive participants with filler voices, withholding the information that some of the talkers in the study might be bilingual, and ultimately reporting that, "[t]here was no indication that any *S* became aware of the fact that bilingual speakers were used" [@lambertEtAl1960, p. 44]. @pharaoKristiansen2019 [p. 2] note that researchers, across both psychology of language and sociolinguistic traditions, go to great lengths to ensure this lack of awareness.

One perhaps surprising, but recurring, demonstration of the two distinct uses of the term perception described here is that, when both levels are examined in the same study, listeners' low level perceptions and high level evaluations need not agree. @mcgowanBabel2020, for example, found that listeners' performance on an AXB vowel discrimination task and their answers in a subsequent interview about the voices heard in that task sometimes agreed, but sometimes diverged. When they diverged, the low level perceptions tracked vowel categories established by the listeners' initial experience with the voice, while high level evaluations of the talker much more closely tracked language ideologies regarding the Quechua-dominant or Spanish-dominant speaker social labels provided by the experiment. Indeed, several participants explicitly commented on the differences between the fricatives used by the two guises; speech sounds that had been held identical in the stimuli. McGowan and Babel attempt to demonstrate, through analysis of participants' explicit commentary on the AXB task, that participants *believed* the guise manipulation, but the stark difference between performances on the AXB discrimination task and evaluative commentary on the voice in each guise, particularly in the second guise, leave open the possibility that listeners became aware of the guise manipulation and were responding out of politeness or a desire to do well in the experiment. 

In an early use of the MGT to study listeners' evaluations of regional accents in the UK and the Republic of Ireland, @milroyMcClenaghan1977 employed four speakers to each perform their own single accent: Received Pronunciation, Ulster, Dublin, or Scottish. They note that Lambert's bilingual investigation, in which, "unknown to the judges a single speaker was heard
in different guises... seems more suitable for use in the bilingual situation where it was originally developed than for use with different accents." (p. 2). The methodological consideration here is one of control rather than awareness on the part of both speaker and listener. Milroy & McClenaghan express "grave reservations" that a single talker, even a talented mimic, could authentically control all four of the regional varieties to be evaluated. Unstated in this preoccupation with production is the corresponding concern that listeners will not *believe* the mimicked accents.

The predominantly protestant Ulster listeners in this task provided both subjective evaluations of the voice quality of each talker on 8 personal characteristics such as intelligence, generosity, and friendliness and were asked to name the region associated with each voice. While the personal characteristics ratings closely tracked expected ideologies for an Ulster judge responding to a Scottish, RP, Dublin, and Ulster accent, the participants proved almost entirely incapable of correctly labeling each variety [see also @campbell-kiblerIssue; @clopperPisoni2004; @kristiansen2009]. Milroy and McClenaghan suggest in their conclusion that perhaps accent identification "takes place below the level of conscious awareness," with implicit stereotypical associations of a given accent arising in the listener independently of a conscious ability to explicitly name that accent.

This recurring disjunction in listeners' implicit and explicit responses, even within a speaker evaluation paradigm, points to what [@kristiansen2009, p. 169] has described as "layers of consciousness" and motivates [@BabelIssue] to describe perception as a "complex, multi-layered process." The picture that is emerging is one of simultaneous, layered complexity in the interactive process of social meaning making. A listener to even a single spoken word combines multi-modal sensory information, their own experiences with language, their own experiences with social meanings, their stereotypes, and their context-driven expectations about the voice they are likely to hear, the words that voice is likely to produce, and the socioindexical properties that voice is likely to embody. And rather than the outcome of perception (broadly construed) being a simple lexical item, a set of speech segments, a single attitude, or a summary evaluative judgment, the listener's subjective experience appears to be a rich, potentially contradictory, superposition of all of these and more.

The Matched Guise technique has been deployed in numerous configurations, but, at its core, the technique almost always employs a single linguistic signal, such as an identical talker [e.g., @giles1970], identical recordings [e.g., @Niedzielski1999], identical texts with multiple talkers [e.g., @milroyMcClenaghan1977], or some combination of these. The manipulated variable in the linguistic signal may be presumed to be unavailable to conscious introspection [@Bender2005; @Donofrio2018] or a stereotype, available to metalinguistic commentary [@campbell-kibler2005; @Squires2013]. This signal is paired with multiple purported social categories to investigate the influence of those categories on participants' evaluations [@campbell-kibler2005; @campbell-kibler2007] or language attitudes [@hadodoIssue; @chan2021]. In social, segmental speech perception research, cross-modal audio/visual extensions of the MGT are common in which visual information serves as a 'guise' for identical voice recordings [@campbell-kibler2016; @gnevsheva2017; @McGowan2015; @haywarrendrager2006]. This type of guise manipulation has been called ‘inverted’ matched guise [@McGowan2015] or simply ‘identification’ [@drager2013]. The MGT has traveled far from its original context of bilingual evaluations, but uniting these linguistic researchers and delineating them from colleagues in social psychology [for discussion, see @rosseelGrondelaers2019], is the foundational methodological assumption that the connection of voice to social type is available to participants' introspective awareness, even when the variable under investigation is not, and therefore requires that listeners not become aware of the guise manipulation.

A central focus of @mcgowanBabel2020 [pp. 246-248]'s discussion, particularly of their interview results, centers on the question of whether deception was successful and listeners *believed* the two MGT guise manipulations. In part this is because they observe a stark disjunction between the segmental and evaluative levels of perception. Belief is especially important in a paper that reports the outcome of an unusual within-subjects MGT which presents both guises to each participant. But more fundamentally, and of interest to anyone employing the MGT for language perception or evaluation research, the assumption of belief, of the requirement that listeners not become aware of the deception inherent in whatever version of the signal/social label guise manipulation being deployed, is at the core of the MGT and has been from the beginning.

However, the majority of studies cannot speak directly to this lack of awareness during segmental perception because the data provided by the participants is relatively late in processing and involves layers of potential introspection and evaluation that block access to the initial online percept for listeners and researchers alike. @Niedzielski1999 infers, on the basis of later, evaluative judgments of diphthong onset quality, that social information has blocked online access to phonetic detail, but this can not be confirmed by the task. Additionally, @mcgowanBabel2020 suggest that inferring segmental perception behavior from evaluative perception tasks is unwise given how dramatically responses to these two levels may disagree [see also @campbell-kibler2012]. To understand how awareness of the guise manipulation may influence perception behavior, the present study uses the inverted MGT to test listeners' segmental perceptions of an [ʃ]-[s] fricative continuum under both different guise and different awareness conditions.

### Segmental perception: [ʃ]-[s] perception {#sec-fricative-gender}

Listeners perceive a greater proportion of an [ʃ]-[s] continuum as [s] if they believe the talker to be male [@strand1999], but the acoustic and sociophonetic motivations for why this might be have emerged slowly over nearly 50 years of research and have often been burdened by the assumption that the phonetic properties of gender are simple, automatic, and biologically determined [@johnson2005]. In the following two subsections, we will lay out our understanding of the relationship between this phonetic variation and its social interpretation as a form of interactive social meaning-making.

Articulatorily, these fricatives mainly differ in constriction width (the extent of apical contact) and place (the distance between the point of lingual articulation and the teeth). The size of the resulting space behind the teeth gives these sounds their characteristic sibilance [@fant1960; @shadle1991]. English [s] has a short resonating chamber behind the teeth with a narrow constriction. English [ʃ] has a comparatively larger resonating chamber and wider constriction, causing lower frequency noise than an [s] for the same speaker. Concomitant with this articulatory difference for English listeners is a cultural association of masculinity with larger, longer vocal tracts and femininity with smaller, shorter vocal tracts [@may1976; @ohala1994; @eckert2012]. In the aggregate, [s] produced from a larger vocal tract will typically be lower in frequency than an [s] produced from a smaller vocal tract, and listeners know this [@may1976]; although sociophonetic differences and biological differences both contribute to observable patterns of gendered fricative production in English [@fuchsToda2010]. This gendered fricative effect is, in practice, entirely separable from between-speaker differences in fundamental frequency (F0) and, like F0, can be used to perform and perceive gender identity.

A commonly used methodology in speech perception research involves the creation of synthetic fricative continua. These continua have endpoints in prototypical examples of [ʃ] and [s], with some number of acoustic steps spliced, synthesized, or even mixed between these. Near the middle of such a continuum will be a synthetic fricative that is ambiguous as to category membership: not clearly [ʃ] and not clearly [s] for the vocal tract that produced the endpoints. @may1976 paired such a continuum from [ʃ] (centered at 2.9 kHz) to [s] (centered at 4.4 kHz) with synthetic [æ] vowels to form simple CV syllables. May found that listeners perceived a higher proportion of the fricative continuum as [ʃ] when paired with vowel stimuli from a smaller vocal tract. The logic here is that smaller resonating chambers between the lingual articulation and the teeth will have a higher mean frequency than larger resonating chambers. Listeners' use of apparent vocal tract size in perception reflects their knowledge of this variation [@munson2011].

Listeners are so acutely sensitive to the alignment of these acoustic facts and cultural associations that perceived gender and fricative category participate in a relationship that is highly reminiscent of a phonetic trading relation [@repp1982]. Not only can believing that a talker identifies as male lead listeners to perceive more [ʃ]-like sounds as [s] [@strandJohnson1996; @munson2011], but a lower fricative consistent with a larger vocal tract is perceived as more masculine [@bouavichithEtAl2019], resulting in more looks to a prototypically male face than a prototypically female face when the task is listening to a word and answering "who do you hear?"

@strandJohnson1996 conducted a pair of experiments investigating the influence of the purported gender of a talker on segmental perception. In their first experiment, listeners heard a [ʃ]-[s] continuum paired with voices that had been previously normed as prototypically female, non-prototypically female, prototypically male, and non-prototypically male. Their result replicates and extends previous work [@may1976; @MannRepp1980] to show that the influence of a gendered voice on segmental perception correlates with the gender-protypicality of that voice. Their second experiment finds that presenting listeners with prototypically-gendered videos of their purported talker can, again, shift perceptions of the [ʃ]-[s] continuum such that listeners report hearing a higher proportion of the continuum as [ʃ] when watching a female talker and a higher proportion of the same continuum as [s] when watching a male talker. 

## Phonetics, Speech Perception, and the Social-Construction of Gender {#sec-gender}

It has long seemed normal in phonetics to imagine that gender is a simple, automatic projection from biological sex onto social identity [@daniel2007; @samolinski2007; @sawusch2005] that listeners can simply normalize away [@johnson2005] to facilitate linguistic perception. Even within sociolinguistics and sociophonetics, where conceptualizations of gender have long been more nuanced, perception research has "retained a basically binary view of gender" [@campbell-kibler-miles-hercules2021, p. 52]. This may be due to the simple expedient that experimenters need our stimuli to *work* for a large cross-section of listeners despite tremendous individual difference and cultural mismatches in both the range of gender categories  and the fine phonetic detail available for the production and perception of those categories [@eckertPodesva2021]. Listeners can only make use of this phonetic variation if it is indexed for them in experience or ideology [@Drager2010b]. Unfortunately, it can be essentially impossible for an experimenter to tell the difference between a genuine finding that a particular social variable does not influence perception, on the one hand, or an indexical mismatch between experimental stimuli and participants' indexical inventories [@barrettHall2024] on the other. The creation and use of stimuli that are rated as highly gender-prototypical for a large group of listeners therefore maximizes the probability that a perception experiment will find an interpretable result.

<!-- However, it can not be over-emphasized that an essentially binary view of gender is inconsistent with the available evidence. We might, for example, expect to see differentiation emerge only at puberty. It does not. In fact, prior to the onset of puberty, girls' oral and nasal cavities tend to be larger than those of boys [@samolinski2007]. If anything, we should expect lower formants and lower center and peak fricative frequencies for girls, inverting the adult pattern. Instead what we observe is that listeners can differentiate the voices of children as young as 4 years of age using vowel formant frequencies [@perryOhdeAshmead2001] and other cues. @schellingerMunsonEdwards2017 report a pair of experiments in which participants heard words produced by children between the ages of 2 and 5, and provided continuous ratings identifying fricatives, vowels, and gender typicality. Children typically show gendered patterns in speech at age 4 and up despite vocal tract length being non-distinct for this cohort. And even in adult speakers, fundamental frequency is only a weak predictor of size, explaining less than 2% of observed height and weight differences while formant spacing fared only slightly better at roughly 10% [@pisanski2014, p. 94].
-->

Therefore, it can not be overemphasized that an essentially binary view of gender is inconsistent with the available evidence: gendered variation in such phonetic cues as fundamental frequency, formants, and fricatives is not purely the result of vocal tract biology but also gestural coordination and performance. Small variations attributable to secondary sex characteristics become available as the semiotic building blocks of gender identity. People who identify as male, female, non-binary, intersex, etc. perform that identity through gestural style.  Trans men, even while experiencing the very real physical consequences of hormone treatments, may also adopt masculinizing alternations to their speech gestures to achieve their ideal gendered voice [@zimman2018]. Gender is more likely the product of, rather than an explanation for, linguistic variation [@eckertPodesva2021]. Just as with words, genders are arbitrary; both the social labels and their acoustic correlates are language specific, and the constellations of meanings are socially-constructed in interaction [@eckert2008].

Vowels, in both their linguistic and social aspects, are the acoustic consequence of gestural control. The formant ratios that distinguish 'male' from 'female' in Norwegian are markedly different from the formant ratios that do this in Danish [@Johnson2006]; what it means to be 'male' versus 'female' is quite different in Thailand than in Japan [@kang2013; @alpert2014]. Children don't perform adult-like vowel formant patterns because they were born tiny men and women, children perform adult-like vowel formant patterns because they are socialized into gender and are using the cultural and linguistic resources available to communicate that gender to others just as adults do. Humans are meaning-making agents, not deterministically resonating meat tubes.

In the earliest sociophonetic perception research, it was still possible to imagine that the kind of knowledge listeners drew on to perceive gender was knowledge of primary biological traits. We now understand, on the contrary, that the influence of gender-based expectations in speech perception is evidence of the influence of cultural knowledge on what might previously have been construed as purely linguistic decisions [@boydfruehwaldhall-lew_2021].

One goal of the present study is to take advantage of the sociophonetic trading relation between listeners' gender and fricative categories to explore the role of awareness in socially-informed speech perception. It is well established that social information can influence how listeners perceive [@foulkesDocherty2006], retrieve [@walkerHay2011], and even remember [@nygaard1994] the linguistic aspect of the speech signal. However, because our accounts of these phenomena come from disparate intellectual traditions, working with a range of quantitative and qualitative methods, with differing assumptions about the role of introspective awareness during the integration of social and linguistic information [@babelCampbell-kiblerMcGowanIssue], one can come away from a detailed, rigorous review of the sociolinguistics, linguistic anthropology, and phonetics literature simultaneously convinced that listeners' use of social information happens both obligatorily above and below the level of conscious awareness.

This paper reports an audiovisual matched guise experiment with both standard 'hidden' and novel 'unhidden' instruction conditions. The basic task is a replication of @strandJohnson1996. Listeners are asked to identify an ambiguous word as *sack* or *shack* on a [ʃ]-[s] continuum given manipulated beliefs about the gender identity of the talker [@trippMunson2022; @steckerDOnofrioIssue]. Numerous previous replications have found that listeners perceive more of the ambiguous continuum as [ʃ] when they believe the speaker identifies as a woman and more as [s] when they believe the speaker identifies as a man, and that, furthermore, this effect is bi-directional, with fricative type influencing perception of gender for an ambiguous voice [@bouavichithEtAl2019]. Unusually, participants in the present study's 'unhidden' instruction condition were briefed in the instructions about the guise manipulation. They were instructed that the man or woman in the photo was not associated with the voice they were listening to. [@campbell-kibler2020], using a similar manipulation, finds that listeners have some ability to disregard social information when making accentedness or attractiveness judgments, but that influence of available social information, particularly from the voice, is difficult to disregard completely. In the present study, participants were asked to provide a *sack*/*shack* lexical decision
either with, or without, explicit instructions to disregard the visual stimulus.

# Method {#sec-method}

## Participants {#sec-participants}

120 participants (self-identified: 59 female, 61 male; ages 20 to 75) were recruited to complete the experiment online. These
participants were recruited through prolific.com and had provided language history and demographic data as part of Prolific's general pre-screening questionnaire. Participation was restricted to a standard sample of desktop computer users located in the USA, who spent most of their childhoods in the US, spoke English as their first and primary language, and having no known language or hearing difficulties. Additionally, due to an audio playback restriction imposed by Apple Computer, the Safari web browser could not be used. Participants were urged only to accept the task if they could do so in a quiet space, free from distractions, and wearing headphones for the 6 to 10 minute duration of the experiment (average time: 6:51). Headphone usage was not verified within the instrument. No participants' data were excluded from analysis.

Participants were paid \$3 for their time, prorated from a projected rate of \$20/hour (actual rate: \$26.29/hour). This same instrument was piloted in the Speech Perception lab of The Ohio State University, and, while reaction times online were generally slower than in-person, results from the online administration were generally consistent with pilot results collected under laboratory conditions. Four participants were excluded for low accuracy rates (below 85%).

## Stimulus Materials {#sec-stimuli}

### Auditory Stimuli {#sec-stimuli-auditory}

The auditory stimuli used in this study are the same wav-format files used in [@bouavichithEtAl2019]. The stimuli, which were generously shared with us, contain two parts, both of which are drawn from synthetic continua: a fricative onset and a VC rime. The fricative onsets comprise a synthetic six step /ʃ-s/ continuum. These steps were generated with the Klatt Synthesizer in
Praat [@praat2001] using parameters from @munson2011 ranging between the values of Munson's second and eighth continuum steps (which were, in turn, based on the parameters used in @strandJohnson1996). Centers of Gravity ranged from a low of 3.2 kHz (/ʃ/-like) to a high of 7 kHz (/s/-like). This continuum is essential to the design as it will allow us to observe any influence of purported gender on listeners' behavioral responses.

For the VC rime, two additional continua were modified from natural productions of [æk] spoken by cisgender male and female talkers in the carrier phrase "Say sack again." These five-step rime continua were created by evenly spacing mean F0 across consecutive steps such that the male-spoken /æk/ continuum increased F0 frequency and formant spacing from their unmodified values in a feminizing direction. Conversely, the female talker's /æk/ continuum decreased both parameters from her unmodified productions to create a continuum in a masculinizing direction. These continua essentially allow us to combine the designs of Strand & Johnson's experiment 1 and experiment 2 in a single task. Listeners will be presented with a wide range of phonetic information from the unmodified gender-prototypical starting points through a range of increasingly non-prototypical continuum steps.

Following the separate creations of these continua, each synthesized fricative token was concatenated with each CV rime of /æk/, resulting in a total of 60 unique auditory stimuli. Each fricative step + rime step stimulus item was played independently as the auditory stimulus item in the perception experiment. These manipulations are described in greater detail in Bouavichith et al.'s Section 2.1 and are summarized visually in @fig-stimuli. Unlike MGT studies that ask a talented, multi-dialectal talker to consciously change their speech style [e.g., @wright2023], these stimuli were produced by one female and one male talker who were asked to record speech in their normal voices. As these talkers were advanced doctoral students in a linguistics PhD program, some of the elements of such an identity are likely available to conscious reflection, but many of these indexical features (such as VOT duration, F2:F3 formant ratio, etc.) are likely implicit, unavailable for conscious control, even for them. 

<!-- https://excalidraw.com/#json=HfAVVSO_awoiHuH4qokTI,p-Qam4bUMuEFXux77dnmVQ
-->

![@bouavichithEtAl2019 auditory stimulus continua. S1, S2, S3, S4, S5 and S6 represent six continuum steps from most *sack*-like to most *shack*-like fricatives. F0 and Formant spacing ratio plots show the manipulations to the Male and Female voiced vowels across five coda steps.](images/figure1.png){#fig-stimuli width=80% fig-align="center"}


## Explicit Evaluations of Auditory Stimuli {#sec-stim-evals}

Because voices carry social information, we elicited explicit social ratings to better understand how the auditory stimuli might influence participants' perception of the identities of the two talkers.  It is important to remember that these explicit evaluations are, themselves, evaluative judgments and will not be taken as veridical evidence of how listeners will experience the voices during segmental perception (see @sec-mgt). 40 undergraduate students at the Ohio State University (25 female, 15 male, ages 18-26) who participated in an in-person pilot version of the inverted matched guise experiment were asked to make judgments regarding the gender, gender prototypicality, and sexuality of a natural, unresynthesized production of *sack* produced by each of the two talkers. Participants listened to the recording and then selected from a fixed set of responses; no free form responses were elicited.

Participants' judgments of the female voice indicate general agreement about the gender identity of the speaker. Most participants (93%) indicated the speaker's gender to be female (2 participants further specified 'trans-female'), and 3 were unsure or otherwise unable to determine the speaker's gender. For the female voice, average prototypicality ratings (in which, for a given gender, 0 is least prototypical, and 5 is most prototypical) were 4.3/5 if the participant had indicated 'female', and 2.75/5 if the participant had indicated 'trans female'. Judgments of the voice's sexuality were more variable, with 54% indicating they were unsure, 40% indicating the speaker was most likely heterosexual, and 1 participant each indicating the speaker was most likely bisexual or another sexuality.

Participants' judgments of the male voice suggest similar agreement. 80% of participants indicated the speaker's gender to be
male (1 further specified 'trans-male'), and 21% were unsure of the gender of the speaker. Average prototypicality ratings were lower for the male speaker but similarly consistent: 3.6/5 if the participant had indicated the voice belonged to a 'male' speaker, and
2/5 if they had indicated the person speaking was a 'trans male'. As with the female voice, judgments of the voice's sexuality were more variable. 65% indicated they were unsure, 14% indicated the speaker was most likely heterosexual, and 16% indicated homosexual, and, again, 1 each indicating the speaker was most likely bisexual, or another sexuality not listed. Crucially, no participants rated the female voice as male or the male voice as female. The variation among ratings is likely due to the presentation of options beyond binary female and male categories and/or to the current cultural understanding of gender performance as distinct from sex. Despite this variability in responses, no 'implausible' answers were given. All things being equal, it is reasonable for a listener to believe there may be little perceptual difference in cis and trans voices for either male or female performances [@zimman2018], and reasonable to consider 'unsure' the most acceptable option in lieu of asking the talker for their gender identity.

### Visual Stimuli {#sec-stimuli-visual}

The visual stimuli used in this study, again identical to the images used in [@bouavichithEtAl2019], are shown in @fig-visual.
These included two face images used for the guise manipulation, which were retrieved from the Chicago Face Database [@ChicagoFaceDatabase], a resource containing high-resolution, normed images of faces indexed by gender and ethnicity. The faces selected were normalized for both physical attributes (i.e., measurements of particular facial dimensions), subjective ratings such as attractiveness, and for gender and gender prototypicality. As in Bouavichith et al., CFD-WF-015-006-N
was selected as the representation of the gender-protypical female talker and CFD-WM-029-023-N was selected as the representation of the gender-prototypical male talker. Both images were converted to greyscale at the command line using ImageMagick [@imagemagick].

Additionally, two gray-scale line drawings were used as visual representations of *shack* and *sack*. These images were used in place of orthographic targets both to maintain consistency with Bouavichith et al's design and to facilitate future eye tracking investigation of this phenomenon. This is a divergence from the original Strand & Johnson design, which represented target words orthographically.


![Stimuli comprised *shack* and *sack* targets (top) and a gender-protypical 'male' and 'female' face (bottom)](images/facesanddrawings.jpg){#fig-visual width=80% fig-align="center"}


## Procedure {#sec-procedure}

The experiment was created in OpenSesame v3.3 [@opensesame] and exported
for the web using OSWeb v1.4.14.0. Modifications to the experiment
included translating portions of the Python code into JavaScript and
adding code to collect Prolific IDs and provide proof of completion to
Prolific at the end of the experiment. This experiment was hosted on a
JATOS [@JATOS] instance hosted on an Ohio State University Linguistics
Department server. Participants received a link to the experiment via
Prolific and used their own computers, keyboards, and headphones to
complete the experiment.

In a between-subjects design, participants were randomly assigned to one
of two awareness conditions. These conditions differed only in the
initial information provided as to the nature of the experiment.
Participants in the *hidden* condition experienced a standard Matched
Guise task. They were given no information about the task or the
stimulus materials beyond the general instructions for completing the
experiment: listen to the voice, press 'z' if you heard the word on the
left, press 'm' for the word on the right. Participants in the
*unhidden* condition also received this instruction and were given a
partial debriefing regarding the task. They were informed that-- while
they would see faces onscreen while hearing words-- the voices in a
given trial were not produced by the person shown in the images; the
images had been downloaded from a database of photographs created at the
University of Chicago for experimental use, and that the auditory and
visual stimuli were in no way related to each other. Participants were
divided equally among these two conditions. Neither awareness condition
was informed about the synthetic nature of the auditory stimuli.

Additionally, participants were assigned to one of two gender congruity
conditions. Although the manipulated rimes sounded gender ambiguous to
us and had been rated as ambiguous by [@bouavichithEtAl2019]'s pilot
participants, the possibility remained that the voices, particularly at
the endpoints, might be perceived incongruously with the faces, as in,
for example, [@McGowan2015][^2].

[^2]: We are choosing the words 'congruous' and 'incongruous' [@schulman1974] intentionally to suggest faces and voices may pattern together in particular ways in listeners' experience and perception with no implied claim that voices may 'match' or 'mismatch' in some way that suggests either experimenters or participants have veridical access to an objective reality.

In congruous trials, the faces and voices were paired such that
participants were only presented with auditory stimuli from the female
talker's continuum alongside the female face, and tokens from the male
talker were only presented alongside the male face. In incongruous
trials, by contrast, auditory stimuli from the female talker's continuum
were only ever presented alongside the male face, and tokens from the
male talker's continuum were only ever presented alongside the female
face. Half of participants were randomly assigned to each congruity
condition, resulting in a 4-way between-subjects design across
instruction and congruity conditions. Each participant heard all 60
auditory stimuli; 30 paired with the male face and 30 paired with the
female face.

In each trial, participants were shown one of the two faces for 1500 ms.
Following this initial presentation, the face remained onscreen and was
flanked by the *shack* and *sack* images. Simultaneously, one of the
auditory stimuli was played over the headphones. The trial ended when
the participant pressed an appropriate key on their physical keyboard,
and their response and reaction time data were uploaded to the JATOS
instance. In both congruous and incongruous conditions, all 60 unique
trials (30 per face) were presented twice to each participant for a
total of 120 trials.

# Predicted Results {#sec-predictions}

## Face: male or female {#sec-pred-face}

Consistent with previous results, we expect to replicate the Strand
effect; in general, we anticipate that more of the [ʃ]-[s] continuum
will be heard as [ʃ] when participants are shown the female face and
more to be heard as [s] when participants are shown the male face.
However, these general predictions about the Face presentation when the
congruence of auditory and visual components of the guise are taken as a
whole.

## Congruence: pairing of face and voice {#sec-pred-congruence}

To our knowledge, the influence of congruence has not been directly
investigated for listeners' joint perception of gender and fricative
place. [@johnsonstranddimperio1999] tested AV integration of Male and
Female faces with prototypical and non-prototypical gendered voices in a
vowel quality perception task. They find what appears to be an
incongruence effect with the prototypical male voice; listeners reported
no difference in perceived vowel quality with this voice in either Face
condition [@johnsonstranddimperio1999 p. 376]. For this reason,
we anticipate a replication of the Strand effect on fricative
identification in our congruous trials (when Face and Voice do not
conflict) but a failure to replicate for the incongruous trials (when
Face and Voice provide conflicting social information). This difference
may be stronger with the male voice, given both Johnson, Strand, and
D'Imperio's finding [see also @king2021].

We make a similar prediction for reaction times.
[@johnsonstranddimperio1999] did not collect reaction time data, but
[@McGowan2011] reports longer reaction times for incongruous trials,
albeit in a very different task, and [@whalen1984] results with subcategorical, coarticulatory mismatches for fricative identification would seem to suggest
that this should hold for listeners' identification of fricatives on a
[ʃ]-[s] continuum. Specifically, we predict longer reaction times,
in general, for the Incongruous conditions. Furthermore, when gender
information is most clear, at gender continuum steps 1 and 2 for the
Male talker and at gender steps 4 & 5 for the Female talker, and in
conflict with the presented Face, listeners' response times should be
slower.

Since strong phonetic correlates of gender, F0 and F3, have been
manipulated over the course of the VC rime continua in our auditory
stimuli, we anticipate that the effect of incongruous face and voice
should be strongest for the natural end points of the continua where the
difference is most salient and weaker as phonetically-cued gender
information becomes more ambiguous. These stimuli have been
independently normed for ambiguity [@bouavichithEtAl2019 p. 1040] in the 2nd and 3rd levels of the rime continua. This means we
anticipate an interaction between Face and Rime step but only in the
incongruous trials and only at the extremes of the rime continuum.

## Guise: Hidden or Unhidden  {#sec-pred-guise}

The primary goal of this experiment was to explore the role of listener awareness and control in the matched guise technique. The care researchers take to ensure that the guise manipulation is hidden from participants suggests a kind of imagined fragility of
the effects of social information on language perception. From this view: listeners who become aware of the guise manipulation will have introspective access to and deliberative control over the influence of visual social information on perception. If this is true, explaining the guise manipulation, in the unhidden condition, should have a strongly negative effect on the Strand effect. Alternatively, if the influence of social information is not available to introspection or deliberative control, we should see no change between the (traditional) hidden matched guise and the unhidden guise.

Additionally, we speculate that there may be a response time difference between the Hidden and Unhidden guises even if there is no apparent difference in percept between the conditions. It can certainly be the case that participants will arrive at the same behavioral responses via different cognitive processing paths, perhaps drawing on different levels of knowledge and awareness, and that these differences may be visible in response times between the Instruction conditions.

# Results  {#sec-results}

Participants provided a total of 14,400 trials (120 trials from each of 120 online participants; 3600 trials in each instruction
x congruity condition). It is not clear what it means to be 'accurate' when asked to perceive fricatives from a continuum, so accuracy was calculated only for responses to the [ʃ] and [s] endpoints. Overall, participants were highly accurate (96.8%) but four participants were excluded from further analysis for accuracy below the predetermined 85% threshold reducing the total number of trials to 13,920. Trials were coded as correct if the participant responded 'shack' to onset step 1 or 'sack' to onset step 6. The four excluded participants all scored 67.5% accuracy or lower.

An additional 50 trials were excluded due to response times that were either too fast or too slow. To reduce the effects of response time outliers on subsequent analyses, all response times shorter than 50 ms (N=0) and longer than 5000 ms (N=50) were excluded. The 5000 ms response time cutoff was used instead of imposing an in-experiment time limit on responses to a trial to ensure that participants were required to respond to each trial. Altogether, 530 trials were excluded, leaving data from 13,870 trials for analysis (approximately 96.3% of the initial data set). The majority (96.8%) of the remaining response times were within a range between 200 and 2000 ms. To increase normality of the distribution of response times across participants, the remaining response times were log-transformed.

## [ʃ]-[s] Percepts  {#sec-results-fricative}

@fig-scurve presents listeners' percepts on this 2AFC task. The horizontal axis in each of these four plots is the fricative
(syllable Onset) continuum step. Step 1 of the continuum is most [ʃ]-like, step 6 is the most [s]-like, steps 3 & 4 are the most
ambiguous. Darker lines in @fig-scurve present trials using the female Face; lighter lines present trials using the male Face. The Hidden and Unhidden instruction conditions are represented by the left and right columns of figures, respectively. The rows present the Congruous blocks where Face and Coda speaker voice shared a gender identity (top) and Incongruous trials where Face and Coda speaker voice mismatched in gender identity (bottom).

![*sack* responses plotted as a function of [ʃ]-[s] fricative (Onset) continuum steps and purported gender presented by the face.](images/Scurve.png){#fig-scurve fig-align="center" width=80%}


A successful replication of the Strand effect would mean that a higher proportion of the ambiguous stimuli would be heard as [s] when the purported gender suggested by the face is male than when the face is female. This pattern appears to hold in both the Hidden and Unhidden conditions, but only when gender identity of the talker who produced the CV rime stimuli was congruous with the gender presented in the visual portion of the guise. From @fig-scurve it would appear that listeners' reported percepts more closely track the voice of the talker than the face in the picture when these sources of information are incongruous.


!['sack' responses on ambiguous fricative trials
plotted as a function of CV rime continuum steps and gender identity of
stimulus talker.](images/ambiguous-by-rime-step.png){#fig-rimes fig-align="center" width=80%}


Given that our stimuli included manipulations of F0 and formant spacing to modify the gender typicality of the male-provided and female-provided recordings, one might expect that the results in @fig-scurve might represent only listener responses to the original, unmodified stimuli, but this is not the case. These four subplots include fricative responses across the entire rime continuum. However, this does not mean the resynthesis was entirely unsuccessful. We predicted that, since these phonetic correlates of gender have been manipulated over the course of the VC rime continua, the effect of incongruence should be strongest for the end points of the continua where the social information presented by the voice is, presumably, most
salient and weaker as phonetically-cued gender information becomes more ambiguous. @fig-rimes suggests that this prediction is at least partially borne out. @fig-rimes plots proportion 'sack' responses to the ambiguous portion of the [ʃ]-[s] continuum (steps 3 & 4) as a function of rime continuum step. The lines plot Speaker, rather than Face, and the meaning of line color has changed in this figure: dark lines represent responses to stimuli originally produced by the male talker and the lighter lines represent responses to stimuli originally produced by the female talker. Step 1 on this continuum, in each of the four subplots, includes the most natural token for the male talker and the most manipulated token for the female talker while step 5 includes the most natural token for the male talker and the most manipulated token for the female talker. As before, columns present the Hidden and Unhidden conditions while rows present the Congruous and Incongruous blocks.

In a 2AFC task with unbiased stimuli, chance is 50%. Responses at the .5
line in @fig-rimes suggest that the ambiguous fricatives remained
ambiguous, while responses that tend to be above this line reflect a
tendency toward [s] percepts and responses that tend to be below this
line reflect a tendency toward [ʃ]. Across all 4 conditions, we observe
a declination from highest-proportion [s] responses in step 1 of the
F0 continua to lowest in step 5. When face and voice were congruous,
virtually all male-voice (and male face) responses are above or at 50%
'sack' and virtually all female-voiced (and female face) responses are
at or *below* 50% 'sack'. This is the same pattern that can be observed
at Onset continuum steps 3 & 4 in @fig-scurve. It is not clear from @fig-rimes
alone if there is any difference at all between the Congruous and
Incongruous conditions. However, it is important to recall about the
bottom row of this figure that male talker responses in the incongruous
trials were presented with a female face while female talker trials were
presented with a male face. Even a weakly-significant Strand effect
would predict that the female talker, particularly on the more ambiguous
continuum steps, should show more 'sack' responses consistent with
having been shown a male face and no such effect is evident in this
plot.

Indeed, a striking feature of these figures ([-@fig-scurve; -@fig-rimes]) is how the apparent influence of gender information flips between congruous and incongruous conditions in the former but remains essentially constant in the latter. Taken together, these plots suggest that cues to gender in the voice are a stronger predictor of listeners' reported percept in this matched guise task than just the purported gender of the face but that, while manipulations of F0 and formant spacing may shift the gender *typicality* of a prototypical female or male voice at this low, segmental level.

<!--
XXX in the conclusion: return to typicality but not gender and talk about how high level evaluative judgments that the female voice 'sounds male' after resynthesis, even judgments made by the experimenters themselves, do not necessarily predict or imply listener reactions to the kind of low level, segmental percept we are eliciting here.
-->

Finally, the main objective of this experiment was to explore the role
of listener awareness in the matched guise technique. Here again, there
may be differences between the congruous and incongruous conditions that
will be better understood through quantitative analysis, but the overall
trend is clear. If there is an effect of explaining to participants that
the voice and face in the matched guise task are unrelated to each
other, that effect is so weak as to be essentially invisible in these
visual interrogations of the data. Categorical responses in the Hidden
and Unhidden instruction conditions appear to be identical.

## Logistic Regression and Quantitative Analysis {#sec-results-stats}

These qualitative assessments of listener responses can be examined
further through quantitative analysis. Through model comparison, we
initially arrived at a logistic mixed model to predict percept with
Congruity condition, instruction condition, Onset step, Face, and Rime
step with interactions for all but Rime step. This model was justified
by model selection but given the notorious difficulty of interpreting a
4-way interaction and the preceding visual interrogation of the data, we
opted to separate Congruence into a pair of 3-way models. Using
`glmer()` [@lme4], we divided the data into congruous and incongruous
subsets and fitted a pair of logistic mixed models (estimated using ML
and BOBYQA optimizer) to predict percept with Instruction condition,
Onset.step, Face and Rime step
(`percept ~ Instruction * Onset.step * Face + Rime.step`). The models
included random intercepts for subject. All categorical predictors were
coded using contrast coding.


![Beta coefficients for listener responses in the
Congruous (black) and Incongruous (gray) logistic regression models
plotted with 95% confidence intervals.](images/coefs_instruction.png){#fig-coefs width=80%}


Beta coefficients for the two separate logistic mixed models are plotted together in @fig-coefs. Terms plotted to the left of the dashed zero line have a negative influence on 'sack' percepts in the model, while terms plotted to the right have a positive influence. As a consistency check, we can observe that the levels of the Onset continuum behave in precisely the expected ways and all levels are statistically significant predictors of percept in both models. Onset step 1 ([ʃ]) is negatively associated with 'sack' responses and significant in both the Congruous ($β=-5.00$, $SE=0.28$, $p < 0.001$) and Incongruous
($β=-4.84$, $SE=0.24$, $p < 0.001$) models. Onset step 5 ([s]) is positively associated with 'sack' responses and significant in both the Congruous ($β=4.35$, $SE=0.22$, $p < 0.001$) and Incongruous ($β=4.12$, $SE=0.19$, $p < 0.001$) models.

As visual inspection of the data suggests, this study includes a
replication of the Strand effect in the Congruous condition. There is a
main effect of Face in the model ($β=-0.22, SE=0.09, p<0.05$). Face is
negatively associated with 'sack' responses suggesting that, with these
stimuli, at least, it is more appropriate to understand the effect of
Face as an increase of 'shack' responses given the female Face. The
inclusion of the interaction term for Onset and Face allows us to see
that the effect of Face is greatest on the ambiguous Onset steps 3
($β=-0.43$, $SE=0.11$, $p < 0.001$) and, to a lesser extent, 4
($β=-0.23$, $SE=0.11$, $p < 0.05$).

However, the Strand effect observed in the Congruous condition is not
attributable entirely to the main effect of Face. Rime F0 is also
significant; Rime level 1, the male end of the continuum, is positively
associated with 'sack' responses ($β=0.61$, $SE=0.10$, $p < 0.001$) as is
Rime level 2 ($β=0.52$, $SE=0.10$, $p < 0.001$). Rime level 3, where the
continuum is most gender ambiguous, is not statistically significant.
Rime level 4, on the female end of the continuum, is negatively
associated with 'sack' responses and significant
($β=-0.49$, $SE=0.10$, $p < 0.001$).

Unsurprisingly, the Strand effect has not been replicated in the
Incongruous condition. As is visible in the bottom row of @fig-scurve, the effect of Face on 'sack' responses is not
significant. The interaction of Onset and Face also behaves quite
differently in the Incongruous model. Onset x Face is negatively
associated with 'sack' responses at Onset step 1
($β=-0.66$, $SE=0.24$, $p < 0.001$) but positively associated with 'sack'
responses and significant at Onset step 3 ($β=0.27$, $SE=0.11$, $p < 0.05$).

Interestingly, the significant effect of Rime observed in the Congruous
model also holds, nearly identically, in the Incongruous model. Rime
level 1, the male end of the continuum, is again positively associated
with 'sack' responses ($β=0.77$, $SE=0.10$, $p < 0.001$) as is Rime level 2
($β=0.41$, $SE=0.10$, $p < 0.001$). Rime level 3 is also not statistically
significant in the Incongruous model. Rime level 4, on the female end of
the continuum, is negatively associated with 'sack' responses and
significant ($β=-0.36$, $SE=0.10$, $p < 0.001$).

Finally, the quantitative analysis of the primary objective of this experiment, exploring the effect of unhiding the matched guise manipulation from participants, largely supports the qualitative analysis. As can be observed in @fig-coefs, there is no significant main effect of Instruction condition in either model. Still, a somewhat more nuanced picture emerges from the interactions of Instruction condition with Onset and the 3 way interaction of Instruction, Onset, and Face in the Congruous trials. The interaction of Instruction with Onset is significant, or nearly so, at every step of the fricative continuum other than the most significant. In the [ʃ]-like portion of the continuum, the interaction with face is positively associated with 'sack' responses at step 1 ($β=0.65$, $SE=0.28$, $p < 0.05$) and 2 ($β=0.44$, $SE=0.18$, $p < 0.05$). The interaction of guise with the most ambiguous onset step is not
significant ($β=0.011$, $SE=0.12$). The interaction of Instruction with
Onset step 4, on the [s] end of the continuum is negatively associated
with 'sack' responses and statistically significant ($β=-0.43$, $SE=0.12$, $p < 0.001$). Instruction x Onset step4 is also negatively associated with 'sack' responses but does not reach significance at the standard alpha level
($β=-0.40$, $SE=0.22$, $p = 0.067$). The 3-way interaction of Instruction x
Onset x Face is positively associated with 'sack' responses at step 2
($β=0.41$, $SE=0.17$, $p < 0.05$) and weakly, but not significantly,
negatively associated with 'sack' responses at step 5 ($β=-0.38$, $SE=0.21$, $p = 0.080$).

There is also no main effect of Instruction in the Incongruous trials.
The 3-way interaction of Instruction x Onset x Face, while justified by
model selection for inclusion in this model, also does not reach
statistical significance. However the 2-way interaction of Instruction
with Onset step is positively associated with 'sack' responses at Onset
step 2 ($β=0.53$, $SE=0.21$, $p < 0.05$) and approaches significance at step
3, where it is weakly positively associated
($β=0.18$, $SE=0.11, $p = 0.095$) and step 5 where it is weakly negatively
associated ($β=-0.32$, $SE=0.19$, $p = 0.086$).

## Response Times {#sec-results-rt}

As with the logistic regression models, we again opted to separate
Congruence into a pair of 3-way models for linear mixed model analysis
of our log-transformed response time data. Using `lmer()` [@lme4], we
reused the congruous and incongruous subsets created for the logistic
regression models and We fitted a linear mixed model (estimated using
REML and nloptwrap optimizer) to predict logRT with Guise, Onset, Face
and Rime (`logRT ~ Instruction * Onset * Face + Rime`). The models
included random intercepts for subject. All categorical predictors were
coded using contrast coding. Beta coefficients for both models are
plotted in @fig-coefs-logRT. Terms plotted to the left of the zero line
are associated with a decrease in log response time while terms plotted
to the right of the zero line are associated with an increase in log
response time. Notably, the longest response times are associated with
the most ambiguous steps of the [ʃ]-[s] onset continuum. Onset step
3 is positively associated with response time and significant in both
the congruous ($β=0.08$, $SE=0.007$, $p < 0.001$) and incongruous
($β=0.0$7, $SE=0.007$, $p < 0.001$ ) models. The same is true of step 4 in
the congruous ($β=0.07$, $SE=0.007$, $p < 0.001$) and incongruous
($β=0.07$, $SE=0.007$, $p < 0.001$) models as well. On the other hand, steps
1, 2, and 5 are all negatively associated with response time and also
significant in both models (see @fig-coefs-logRT).

![Beta coefficients for log-transformed response
times in the Congruous (black) and Incongruous (gray) linear regression
models plotted with 95% confidence intervals.](images/coefs-logRT_instructions.png){#fig-coefs-logRT width=80%}


We predicted overall slower response times in the Incongruous than
Congruous conditions and this prediction is not borne out by the data.
Apart from generally higher variability in the incongruous conditions,
there is no positive or negative trend in response times between the two
Congruity models. For example, within the Incongruous model response
times given the interaction of Onset step 3 \* Face are longer
($β=-0.009$, $SE=0.007$, $p = 0.17$), which would seem to support our
prediction, but response times for Onset step 4 \* Face are shorter
($β-0.02$, $SE=0.007$, $p < 0.01$), the opposite of what we predicted. The
exact opposite pattern appears within the Congruous model where response
times are shorter given Onset 3 \* Face ($β=-0.02$, $SE=0.007$, $p < 0.01$)
but longer given Onset step 4 \* Face ($β=0.04$, $SE=0.007$, $p < 0.001$).
These crossing patterns can be seen in @fig-coefs-logRT.

Given the replication of the Strand effect in the Congruous, but not the
Incongruous conditions described in the previous section, it may be
notable that there is a significant main effect of Face in the Congruous
model where it is negatively associated with response time
($β=0.22$, $SE=0.08$, $p < 0.05$) and not significant in the Incongruous
model.

# Discussion

This study was initially motivated by a desire to understand the role of listener awareness and control in the matched guise technique. How important is it, for example, that listeners believe the guise manipulation [e.g. @mcgowanBabel2020]. We attempted to demonstrate in the introduction that the careful steps researchers typcially take to obscure the nature of the guise manipulation from participants reflects a long-held assumption in the sociolinguistics literature that social knowledge is high-level knowledge, available to introspective control [@campbell-kibler2016] and that awareness of the manipulation might therefore alter or allow listeners to control perceptual responses. The results of the present study are inconsistent with this imagined fragility of the influence of social knowledge. Revealing the nature of the guise manipulation did not significantly influence listener responses in either the congruous or incongruous conditions. Nor did this revelation have a significant influence on response times in either condition.

The finding that the Matched Guise effect holds for speech perception both when hidden from the participant and when unhidden is inconsistent with a model of processing in which social knowledge simply acts as a filter on linguistic knowledge. Social knowledge can influence perception even when listeners are aware that it is likely false. This result
parallels previous results for accentedness and attractiveness judgments
[@campbell-kibler2020]. A similar result may be present, for social
information, in the within-participants guise manipulation of
[@mcgowanBabel2020]. In that study, the authors use participants'
metalinguistic commentaries to assess the extent to which the guise
manipulations were or were not 'believed'. The results of the present
study suggest that such belief may be irrelevant, which lends support to @DragerKirtley2016's (p. 9) proposal that "individuals do not
need to be aware of variation in order for that variation to be socially
meaningful." The present result
also gives additional context to studies demonstrating influence of
social knowledge even when listeners have no reason to expect a guise
manipulation [@Niedzielski1999; @haynolandrager2006; @HayDrager2010]. It
is unclear whether social knowledge will prove to be as resilient to
awareness as the obligatory McGurk effect [@McGurkMacDonald1976], which
persists even when participants actively identify that the face and
voice in the experiment are mismatched [@GreenEtAl1991], but the
suggestion is that it will.

The gender identity of the talker who produced the VC Rime supplemented
Face in the Congruous conditions to make the Strand effect even
stronger; the mechanism may prove similar to the way lip-rounding
accentuates the backness of back vowels. In the Incongruous conditions,
though, listeners' perception of the [ʃ]-[s] continuum tracked the
VC Rimes, even along the synthetic gender continuum, rather than the purported gender of the Face. This pattern was strongest in the least-ambiguous portions of the fricative continuum and
weakest in the most-ambiguous. In a sense, by separating trials by
congruity of face and voice, we have replicated [@strandJohnson1996]'s
two experiments simultaneously. One wonders, looking back at their experiment 2,
whether this classic result was *also* a congruous condition in which
listeners had sufficient gender information from the 'non-protypical' voice to supplement
the purported information from the Face. The non-prototypical male and female voices used in Strand & Johnson's experiment 1 were still perceived as male and female. This congruity finding may provide some insight into recent failures to replicate the original Strand effect [@schellingerMunsonEdwards2017; @wilbanks2022].

The phonetic correlates of gender manipulated in the VC rimes for this
study are F0 and formant spacing. However, these may not be the only cues
listeners are drawing upon with their knowledge of US English. Surely,
F0 and vowel formant spacing *can be* important to listeners, just as
voice onset time and vocal fold vibration can be important cues to the
voicing of /t/ and /d/. But as [@lisker1986] catalogs for those stops, there are 16 cues
to this apparently simple feature in English, any of which might be
sufficient to communicate voicing, but none of which is required. In
the present study, we have used manipulated stimuli that obscure, over the
course of two continua, the gender identity of the talker who
produced the basis token for that continuum. At an explicit level, these
continua *sound ambiguous* to the experimenters in much the way that
[@whalen1984]'s stimuli do not sound obviously mismatched. But our
perception results suggest that listeners are still aware, albeit
implicitly, of the gender identity we have attempted to obscure by
altering these commonly-manipulated phonetic correlates.

# Conclusion

Decades of research since [@strandJohnson1996]'s original finding have
demonstrated that a visual cue can shift fricative perceptions when
paired with an ambiguously-gendered voice (although cf Munson 2017 and
Wilbanks 2022). [@bouavichithEtAl2019] demonstrated with
eye-tracking that this effect is fast and bi-directional. One could come
away from Strand & Johnson's experiment 1 and experiment 2 and subsequent replications
with a theoretical model in which visually-cued social information and
phonetically-cued social information exert equivalent influence on
speech perception. Prototypically-gendered voices can shift perception
of a [ʃ]-[s] continuum, and prototypically-gendered visual
information can as well. However, listeners' behavior in our Congruous
and Incongruous conditions is inconsistent with such a model and
suggests, instead, that when visually-cued and phonetically-cued social
information are congruent, they can enhance one another. If, on
the other hand, these information sources conflict, it is the
phonetically-cued social information that will dominate
[@campbell-kibler2020; @mcgowanBabel2020].

It is unlikely that fricatives are unique in this respect. For example,
the incongruous results seen in this study are, perhaps, predicted by the
lack of Face effect for [@johnsonstranddimperio1999]'s vowel perception
results in exp2 given a stereotypical face (particularly, in that study,
for the male voice). As listeners, we do not have veridical access to
the speech sounds intended by a talker. Instead, we must combine the
speech signal with our phonological knowledge, lexical knowledge, social
expectations, visual input, expectations of the social world
[@BabelIssue], and other sensory information to arrive at a percept. The
implication is that perception is more holistic than is dreamt of in our
phonologies. Category boundaries, whether for speech sounds or social
categories, are fuzzy, and perception needs to be fast. We retain
knowledge of and use detailed social and linguistic knowledge at both
high and low levels of processing.

[@barrett2014 p. 205] writes, "any assumption of essentialism will ultimately marginalize those individuals who do not fit the essentialist understandings of human behavior." It may not feel brutal or reductive to read [@may1976]'s findings about large and small vocal tracts as if they refer to male and female vocal tracts, respectively, but it does necessarily imply that tall, long-necked women and short, squat-necked men need to find some other way of labeling themselves. The idea that male voices come from large bodies and female voices come from small bodies need not be literally true for the phonetic and perceptual correlates of size to become enregistered alongside other features in the creation of gendered personae (D'Onofrio 2020). Our prediction that incongruity in face and voice would slow listener judgments was not supported. It is tempting to interpret this as evidence that, unlike misleading coarticulatory information, listeners are aware of the diversity of gender expression, but this is not a question the current study can resolve.

What the current study can resolve is that the influence of listeners' social knowledge of speech on perception is not delicate. The present result is equally inconsistent with a model that disregards social knowledge entirely and with a
model of speech perception that presumes *all* social knowledge to be
late, high-level, and available to introspective control. Part of what
listeners know when they know a language includes the simultaneous
patterning of 'linguistic' and 'social' information in a shared phonetic
signal. Social knowledge and linguistic knowledge are deeply intertwined in speech
perception.

# References {#sec-references .unnumbered}

::: {#refs}
:::
